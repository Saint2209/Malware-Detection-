{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "#import machine learning essentials\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import model_selection\n",
    "import seaborn as sns  # Python visualization library based on matplotlib provides a high-level interface for drawing attractive statistical graphics\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt  # For graphical representation\n",
    "import time\n",
    "\n",
    "from sklearn import linear_model\n",
    "#warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import ML modules\n",
    "sns.set_style('darkgrid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "# Data acquisition: load csv\n",
    "df = pd.DataFrame(pd.read_csv('Data/Obfuscated-MalMem2022.csv'))\n",
    "# Unlock pandas power (yeah!!!)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "# 1.Column Category is unnecessary for this study as we will simply be looking at classifications\n",
    "df.drop('Category', axis=1, inplace=True)\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "print(\"dropped Category\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "#describe data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "# Seperating the targets from the features.\n",
    "# select all features but target\n",
    "X = df.drop('Class', axis=1)\n",
    "# select only target\n",
    "y = df[\"Class\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "#drop columns with no standard deviation of 0\n",
    "X = X.drop(labels=X.loc[:, X.std() == 0].columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "# Tester function\n",
    "def predict(clf, X_test, y_test):\n",
    "    #Start time prediction\n",
    "    start = time.time()\n",
    "    #predict\n",
    "    y_pred = clf.predict(X_test)\n",
    "    #End time prediction\n",
    "    end = time.time()\n",
    "\n",
    "    prediction_time = (end - start)\n",
    "\n",
    "    #accuracy\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    # #confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d')\n",
    "    # #classification report\n",
    "    # cr = classification_report(y_test, y_pred)\n",
    "\n",
    "    \n",
    "    return {\"Test Accuracy\":acc, \n",
    "            \"Test Prediction Time\":prediction_time, \n",
    "            \"Test Prediction Time/Sample\":(prediction_time/X_test.shape[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "#Modeling\n",
    "def Cross_val(clf, X_train, y_train, X_test, y_test):\n",
    "    #score cross validation\n",
    "    score = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_macro']\n",
    "    #cross validation\n",
    "    kfold = model_selection.KFold(n_splits=10)\n",
    "    cv_results = model_selection.cross_validate(\n",
    "        clf, X_train, y_train, cv=kfold, scoring=score, n_jobs=-1)\n",
    "\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "\n",
    "    test_data = predict(clf, X_test, y_test)\n",
    "\n",
    "    return {\"Accuracy\": (cv_results['test_accuracy'].mean()).round(4),\n",
    "            \"Train Sample\": y_train.shape[0],\n",
    "            \"Average Training Time\": cv_results['fit_time'].mean(),\n",
    "            \"Average Score Time\": cv_results['score_time'].mean()}, predict(clf, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "# Stacked Ensemble Classifier\n",
    "def stacked_ensemble(X_new, y_local):\n",
    "    \n",
    "    test_size = 0.50\n",
    "    seed = 42\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X_new, y_local, test_size=test_size, random_state=seed,)\n",
    "    \n",
    "    # prepare estimators\n",
    "    estimators = [\n",
    "        ('RF', RandomForestClassifier()),\n",
    "        ('CART', DecisionTreeClassifier()),\n",
    "        ('NB', GaussianNB())\n",
    "    ]\n",
    "\n",
    "    # create the ensemble model\n",
    "    ensemble = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
    "    # fit the model on all available \n",
    "    \n",
    "    train_data, test_data = Cross_val(ensemble, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    return train_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "# Correlation Coefficient\n",
    "def correlation_coeffiecient(X_in, y):\n",
    "    #correlation coefficient\n",
    "    corr_matrix = X_in.corr()\n",
    "    upper_tri = corr_matrix.where(\n",
    "        np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "        \n",
    "    # # plot correlation matrix\n",
    "    # sns.heatmap(X.corr(), annot=True, cmap='coolwarm', square=True, fmt='.2f', annot_kws={'size': 25})\n",
    "    # his = plt.gcf()\n",
    "    # his.set_size_inches(100, 100)\n",
    "    # plt.show()\n",
    "\n",
    "    #threshold\n",
    "    threshold = 0.7\n",
    "\n",
    "    #drop columns with correlation above threshold\n",
    "    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > threshold)]\n",
    "    X_new = X_in.drop(to_drop, axis=1)\n",
    "\n",
    "    print(X_new.info())\n",
    "    # #plot new correlation matrix\n",
    "    \n",
    "    # sns.heatmap(X_new.corr(), annot=True, cmap='coolwarm',\n",
    "    #             square=True, fmt='.2f', annot_kws={'size': 25})\n",
    "    # his = plt.gcf()\n",
    "    # his.set_size_inches(100, 100)\n",
    "    # plt.show()\n",
    "\n",
    "    return X_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "#Information Gain\n",
    "def mutual_info(X_mi, y):\n",
    "    \n",
    "    #mutual info\n",
    "    mi = mutual_info_classif(X_mi, y)\n",
    "    mi = pd.Series(mi)\n",
    "    mi.index = X_mi.columns\n",
    "    \n",
    "    # mi.sort_values(ascending=False, inplace=True)\n",
    "    # mi.plot.bar(figsize=(25, 10))\n",
    "    # plt.show()\n",
    "\n",
    "    #threshold\n",
    "    threshold = 0.6\n",
    "\n",
    "    #drop columns with mutual info below threshold\n",
    "    to_drop = [column for column in mi.index if mi[column] < threshold]\n",
    "\n",
    "    X_mi = X_mi.drop(to_drop, axis=1)\n",
    "\n",
    "    print(X_mi.info())\n",
    "    \n",
    "    # mi2 = mutual_info_classif(X_mi, y)\n",
    "    # mi2 = pd.Series(mi2)\n",
    "    # mi2.index = X_mi.columns\n",
    "    # mi2.sort_values(ascending=False, inplace=True)\n",
    "    # mi2.plot.bar(figsize=(25, 10))\n",
    "    # plt.show()\n",
    "\n",
    "    return X_mi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "#variance threshold    \n",
    "def variance_threshold(X_var, y_train):\n",
    "    # #variance threshold plot\n",
    "    # plt.figure(figsize=(100, 6))\n",
    "    # plt.title('Variance of Features')\n",
    "    # plt.xlabel('Features')\n",
    "    # plt.ylabel('Variance')\n",
    "    # plt.plot(X_var.var(axis=0))\n",
    "    # plt.show()\n",
    "\n",
    "    selector = VarianceThreshold(threshold=351.93)\n",
    "    selector.fit(X_var)\n",
    "    \n",
    "    cols = selector.get_support(indices=True)\n",
    "    columns = X_var.columns[cols]\n",
    "    X_var = pd.DataFrame(data=X_var.iloc[:, cols], columns=columns)\n",
    "\n",
    "    #variance threshold plot\n",
    "    # plt.figure(figsize=(20, 6))\n",
    "    # plt.title('Variance of Features')\n",
    "    # plt.xlabel('Features')\n",
    "    # plt.ylabel('Variance')\n",
    "    # plt.plot(X_var.var(axis=0))\n",
    "    # plt.show()\n",
    "\n",
    "    print(X_var.info())\n",
    "    return X_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "#feature selection map\n",
    "feature_selection_map = {\n",
    "    'corr_threshold': correlation_coeffiecient,\n",
    "    'var_threshold': variance_threshold,\n",
    "     'info_gain_ratio': mutual_info,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "#main\n",
    "feature_selectors = ['no_feature_selection','var_threshold', 'info_gain_ratio', 'corr_threshold']\n",
    "scores = []\n",
    "for fs in feature_selectors:\n",
    "    if fs == 'no_feature_selection':\n",
    "        X_new = X\n",
    "    else:\n",
    "        selector = feature_selection_map[fs]\n",
    "        X_new = selector(X, y)\n",
    "    trained_model ,test_results = stacked_ensemble(X_new, y)\n",
    "    \n",
    "\n",
    "    # Append a trained model to selected_models\n",
    "    scores.append({'name': fs} | trained_model | test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "#line plot scores\n",
    "df_scores = pd.DataFrame(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "df_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "df_scores.to_csv('Data/no_val_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "df_scores.set_index('name', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "to_plot = df_scores.copy(deep=True)\n",
    "\n",
    "to_plot.drop(labels=[\"Train Sample\",\n",
    "                              \"Test Accuracy\",\n",
    "                              \"Accuracy\",\n",
    "                              \"Test Prediction Time\",\n",
    "                              \"Average Training Time\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "to_plot.plot(kind='line', figsize=(8, 8)).set(title='Time Difference Between Train and Test Data', ylabel='Time (s)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "#plot change in time for each feature selection method\n",
    "ax = df_scores.plot(kind='bar', figsize=(10, 50), subplots=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rate = df_scores.copy(deep=True)\n",
    "df_rate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "#plot fit time per name using seaborn\n",
    "ax = sns.lineplot(x=df_scores.index, y=df_scores['Average Training Time'])\n",
    "ax.set(title ='Training Time' ,xlabel = 'Feature Selection Method', ylabel = 'Fit Time (s)')\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), '.4f'), (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "#plot prediction time per name using seaborn\n",
    "ax = sns.lineplot(x=df_scores.index, y=df_scores['Test Prediction Time'])\n",
    "ax.set(title='Prediction Time', xlabel='Feature Selection Method',\n",
    "       ylabel='Prediction Time (s)')\n",
    "#label each bar with score\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), '.4f'), (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max =  df_rate['Average Training Time'].max()\n",
    "maxp = df_rate['Test Prediction Time'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_Time = {}\n",
    "feature_selectors = ['no_feature_selection','var_threshold', 'info_gain_ratio', 'corr_threshold']\n",
    "count = 0\n",
    "for row in df_rate['Average Training Time']:\n",
    "    row = abs(row - max )/max * 100\n",
    "    print(row)\n",
    "    Training_Time[feature_selectors[count]] = [row] \n",
    "    count += 1\n",
    "# Training_Time to pd\n",
    "print(Training_Time)\n",
    "count = 0\n",
    "for row in df_rate['Test Prediction Time']:\n",
    "    row = abs(row - maxp)/maxp * 100\n",
    "    print(row)\n",
    "    Training_Time[feature_selectors[count]].append(row) \n",
    "    count += 1\n",
    "    \n",
    "Tt = pd.DataFrame.from_dict(Training_Time, orient='index', columns=['Training Time','Test Prediction Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Plotting the training time and test prediction time for each feature selection method\n",
    "\n",
    "ax = sns.lineplot(data=Tt)\n",
    "ax.set(title ='Percentage Time Difference Between Train and Test Data' ,xlabel = 'Feature Selection Method', ylabel = 'Percentage of Time(%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "#score time vs accuracy plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "ax = sns.barplot(x=df_scores.index, y=df_scores['Test Accuracy'])\n",
    "ax.set(xlabel = 'Feature Selection Method', ylabel = 'Accuracy')\n",
    "\n",
    "#label each bar with score\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), '.4f'), (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3062dad7d99d5be0d3f003f8126f8335e1145e3be86486f88cf97daaa9051385"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
