# %%
# Import essentials and visualisation stuff
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler,Normalizer
from sklearn.impute import SimpleImputer
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn import model_selection
import seaborn as sns  # Python visualization library based on matplotlib provides a high-level interface for drawing attractive statistical graphics
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.preprocessing import MinMaxScaler
from sklearn.svm import SVC
import numpy as np  # linear algebra
import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib
import matplotlib.pyplot as plt  # For graphical representation
%matplotlib inline




# Import ML modules


sns.set_style('darkgrid')


# %%
# Data acquisition: load csv
a = pd.read_csv('Data/Obfuscated-MalMem2022.csv')
df = pd.DataFrame(a)
# Unlock pandas power (yeah!!!)
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)



# %% [markdown]
# 1.Column Category is unnecessary for this study as we will simply be looking at classifications

# %%

df.drop('Category', axis=1, inplace=True)
# df.reset_index(drop=True, inplace=True)
print("dropped Category")


# %% [markdown]
# Seperating the targets from the features.

# %%
# select all features but target
X = df.iloc[:, 0:-1]
# select only target
y = df.iloc[:, -1]

print("Targets and features successfully seperated")


# %% [markdown]
# ***Clean dataset and remove colums with std =  0***

# %%
#clean dataset and remove std=0
X = X.loc[:, (X.std() != 0)]
print("removed std=0")


# %%
X.describe()

# %% [markdown]
# **Perform eda in order to see the shape of the data and decide what further preprocessing is required.**
# 1. Looking for outliers.
# 

# %%
count = 0
while (count < X.columns.size):
    X.iloc[:, count: count + 5].boxplot()
    box = plt.gcf()
    box.set_size_inches(15, 15)
    plt.show()
    count = count + 5


# %%
#look for outliers using histogram
count = 0
while (count < X.columns.size):
    X.iloc[:, count: count + 5].hist()
    box = plt.gcf()
    box.set_size_inches(15, 15)
    plt.show()
    count = count + 5

# %%
Xcolumns = X.columns

# %% [markdown]
# ***Normalising data***
# *The data needs to be normalised as it is evident from the histogram that the data is not spread out.

# %%
#Standardise data
scaler = StandardScaler()
X = scaler.fit_transform(X)
print("Standardised data")

# %%
X = pd.DataFrame(data=X, columns=Xcolumns)


# %%
X

# %%
count = 0
while (count < X.columns.size):
    X.iloc[:, count: count + 5].boxplot()
    box = plt.gcf()
    box.set_size_inches(15, 15)
    plt.show()
    count = count + 5


# %% [markdown]
# ***Scaling***
# 1. Robust scaler: (value = (value – median) / (p75 – p25))As it works best with outlier uses the 75th quartile and the 25th quartile, which is more suited to the standard scaler(value = (value – mean) / stdev) which only looks at the mean. Hence robust scaler is better suited for data with a lot of outliers.
# 
# !!! We want to get rid of outliers as they skew the data in a certain direction and not giving a fair valuation of the results.

# %%
#robust scaling
scaler = MinMaxScaler()
X = scaler.fit_transform(X)
print("Robust scaled data")

# %%
X = pd.DataFrame(data=X, columns=Xcolumns)

# %%
#look for outliers using plot
count = 0
while (count < X.columns.size):
    X.iloc[:, count: count + 10].boxplot()
    box = plt.gcf()
    box.set_size_inches(15, 15)
    plt.show()
    count = count + 10

# %%
#look for outliers using histogram
count = 0
while (count < X.columns.size):
    X.iloc[:, count: count + 5].hist()
    box = plt.gcf()
    box.set_size_inches(15, 15)
    plt.show()
    count = count + 5
    

# %%
X.head(50)

# %%
# correlation matrix
threshold = 0.8

#correlation matrix
corr_matrix = X.corr().abs()

# heatmap
sns.heatmap(corr_matrix, annot=True)
plot.show()

# %%



